{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for manipulating directory paths\n",
    "import os\n",
    "\n",
    "# Scientific and vector computation for python\n",
    "import numpy as np\n",
    "\n",
    "# Plotting library\n",
    "from matplotlib import pyplot\n",
    "from mpl_toolkits.mplot3d import Axes3D  # needed to plot 3-D surfaces\n",
    "\n",
    "# Optimization module in scipy\n",
    "from scipy import optimize\n",
    "\n",
    "# library written for this exercise providing additional functions for assignment submission, and others\n",
    "import utils \n",
    "\n",
    "# tells matplotlib to embed plots within the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read comma separated data\n",
    "def splitRawData(data):\n",
    "    X, y = data[:, 0:-1], data[:, -1]\n",
    "\n",
    "    if len(X.shape)==1:\n",
    "        X = np.c_[X] # convert to a column vector\n",
    "    \n",
    "    y = np.c_[y] #convert to a column vector\n",
    "\n",
    "    return X,y\n",
    "\n",
    "\n",
    "\n",
    "data = np.loadtxt(os.path.join('Data', 'ex1data2.txt'), delimiter=',')\n",
    "\n",
    "X,y = splitRawData(data)\n",
    "m = y.shape[0]\n",
    "\n",
    "X = np.concatenate([np.ones((m, 1)), X], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed mean: [   0.         2000.68085106    3.17021277]\n",
      "Computed standard deviation: [0.00000000e+00 7.86202619e+02 7.52842809e-01]\n"
     ]
    }
   ],
   "source": [
    "def  featureNormalize(X):\n",
    "    \"\"\"\n",
    "    Normalizes the features in X. returns a normalized version of X where\n",
    "    the mean value of each feature is 0 and the standard deviation\n",
    "    is 1. This is often a good preprocessing step to do when working with\n",
    "    learning algorithms.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The dataset of shape (m x n).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X_norm : array_like\n",
    "        The normalized dataset of shape (m x n).\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    First, for each feature dimension, compute the mean of the feature\n",
    "    and subtract it from the dataset, storing the mean value in mu. \n",
    "    Next, compute the  standard deviation of each feature and divide\n",
    "    each feature by it's standard deviation, storing the standard deviation \n",
    "    in sigma. \n",
    "    \n",
    "    Note that X is a matrix where each column is a feature and each row is\n",
    "    an example. You needto perform the normalization separately for each feature. \n",
    "    \n",
    "    Hint\n",
    "    ----\n",
    "    You might find the 'np.mean' and 'np.std' functions useful.\n",
    "    \"\"\"\n",
    "    # You need to set these values correctly\n",
    "    X_norm = X.copy()\n",
    "    mu = np.zeros(X.shape[1])\n",
    "    sigma = np.zeros(X.shape[1])\n",
    "\n",
    "    # =========================== YOUR CODE HERE =====================\n",
    "    \n",
    "    for count in np.arange(1,mu.size):\n",
    "        mu[count] = np.mean(X[:,count])\n",
    "        sigma[count] = np.std(X[:,count])\n",
    "    \n",
    "    for count in np.arange(1,mu.size):\n",
    "        X_norm[:,count] = (X_norm[:,count]-mu[count])/sigma[count]\n",
    "    \n",
    "    # ================================================================\n",
    "#     return X_norm, mu, sigma\n",
    "    return X_norm, mu, sigma\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# call featureNormalize on the loaded data\n",
    "X_norm, mu, sigma = featureNormalize(X)\n",
    "\n",
    "print('Computed mean:', mu)\n",
    "print('Computed standard deviation:', sigma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  featureNormalize1(X):\n",
    "    # You need to set these values correctly\n",
    "    X_norm = X.copy()\n",
    "#     mu = np.zeros(X.shape[1])\n",
    "#     sigma = np.zeros(X.shape[1])\n",
    "    \n",
    "    X_min = np.zeros(X.shape[1]);\n",
    "    X_max = np.zeros(X.shape[1]);\n",
    "\n",
    "    # =========================== YOUR CODE HERE =====================\n",
    "    \n",
    "    for count in np.arange(1,X_min.size):\n",
    "        X_min[count] = np.min(X[:,count])\n",
    "        X_max[count] = np.max(X[:,count])\n",
    "    \n",
    "    for count in np.arange(1,X_min.size):\n",
    "        X_norm[:,count] = (X_norm[:,count]-X_min[count])/X_max[count]\n",
    "    \n",
    "    # ================================================================\n",
    "#     return X_norm, mu, sigma\n",
    "    return X_norm, X_min, X_max\n",
    "\n",
    "\n",
    "\n",
    "# call featureNormalize on the loaded data\n",
    "# X_norm, X_min, X_max = featureNormalize1(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCost(X, y, theta):    \n",
    "    # initialize some useful values\n",
    "    m = y.shape[0]  # number of training examples\n",
    "    theta = np.array(theta)\n",
    "    # theta = np.c_[theta] #convert to column vector\n",
    "    \n",
    "    # You need to return the following variables correctly\n",
    "    J = 0\n",
    "    \n",
    "    # ====================== YOUR CODE HERE =====================\n",
    "    J = np.sum(((X.dot(theta))-y)**2)/(2*m)\n",
    "    \n",
    "    return J\n",
    "    # ===========================================================\n",
    "\n",
    "\n",
    "def gradientDescent(X, y, theta, alpha, num_iters):\n",
    "    # Initialize some useful values\n",
    "    m = y.shape[0]  # number of training examples\n",
    "    theta = np.array(theta)\n",
    "    theta = np.c_[theta] #convert to column vector\n",
    "    \n",
    "    J_history = [] # Use a python list to save cost in every iteration\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # ==================== YOUR CODE HERE =================================\n",
    "        h = X.dot(theta)\n",
    "        grad = (X.T.dot(h-y))/m\n",
    "        theta = theta - (alpha*grad)\n",
    "        # =====================================================================\n",
    "        \n",
    "        # save the cost J in every iteration\n",
    "        J_history.append(computeCost(X, y, theta))\n",
    "    \n",
    "    return theta, J_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta computed from gradient descent: [[340412.65957447]\n",
      " [109447.79646964]\n",
      " [ -6578.35485416]]\n",
      "Predicted price of a 1650 sq-ft, 3 br house (using gradient descent): $[1.80909542e+08]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Instructions\n",
    "------------\n",
    "We have provided you with the following starter code that runs\n",
    "gradient descent with a particular learning rate (alpha). \n",
    "\n",
    "Your task is to first make sure that your functions - `computeCost`\n",
    "and `gradientDescent` already work with  this starter code and\n",
    "support multiple variables.\n",
    "\n",
    "After that, try running gradient descent with different values of\n",
    "alpha and see which one gives you the best result.\n",
    "\n",
    "Finally, you should complete the code at the end to predict the price\n",
    "of a 1650 sq-ft, 3 br house.\n",
    "\n",
    "Hint\n",
    "----\n",
    "At prediction, make sure you do the same feature normalization.\n",
    "\"\"\"\n",
    "# Choose some alpha value - change this\n",
    "alpha = 0.3\n",
    "num_iters = 400\n",
    "\n",
    "# init theta and run gradient descent\n",
    "theta = np.zeros(3)\n",
    "theta, J_history = gradientDescent(X_norm, y, theta, alpha, num_iters)\n",
    "\n",
    "# Plot the convergence graph\n",
    "pyplot.plot(np.arange(len(J_history)), J_history, lw=2)\n",
    "pyplot.xlabel('Number of iterations')\n",
    "pyplot.ylabel('Cost J')\n",
    "\n",
    "# Display the gradient descent's result\n",
    "print('theta computed from gradient descent: {:s}'.format(str(theta)))\n",
    "\n",
    "# Estimate the price of a 1650 sq-ft, 3 br house\n",
    "# ======================= YOUR CODE HERE ===========================\n",
    "# Recall that the first column of X is all-ones. \n",
    "# Thus, it does not need to be normalized.\n",
    "\n",
    "price = np.array([1, 1650, 3]).dot(theta)   # You should change this\n",
    "\n",
    "# ===================================================================\n",
    "\n",
    "print('Predicted price of a 1650 sq-ft, 3 br house (using gradient descent): ${}'.format(price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalEqn(X, y):\n",
    "    \"\"\"\n",
    "    Computes the closed-form solution to linear regression using the normal equations.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The dataset of shape (m x n+1).\n",
    "    \n",
    "    y : array_like\n",
    "        The value at each data point. A vector of shape (m, ).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    theta : array_like\n",
    "        Estimated linear regression parameters. A vector of shape (n+1, ).\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Complete the code to compute the closed form solution to linear\n",
    "    regression and put the result in theta.\n",
    "    \n",
    "    Hint\n",
    "    ----\n",
    "    Look up the function `np.linalg.pinv` for computing matrix inverse.\n",
    "    \"\"\"\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    \n",
    "    # ===================== YOUR CODE HERE ============================\n",
    "    #theta = pinv(X'*X)*X'*y;\n",
    "    theta=((np.linalg.inv(X.T.dot(X))).dot(X.T)).dot(y)\n",
    "    \n",
    "    # =================================================================\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta computed from the normal equations: [[89597.9095428 ]\n",
      " [  139.21067402]\n",
      " [-8738.01911233]]\n",
      "Predicted price of a 1650 sq-ft, 3 br house (using normal equations): $[293081.46433489]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the parameters from the normal equation\n",
    "\n",
    "theta = normalEqn(X, y)\n",
    "\n",
    "\n",
    "# Display normal equation's result\n",
    "print('Theta computed from the normal equations: {:s}'.format(str(theta)))\n",
    "\n",
    "# Estimate the price of a 1650 sq-ft, 3 br house\n",
    "# ====================== YOUR CODE HERE ======================\n",
    "\n",
    "price = np.array([1, 1650, 3]).dot(theta)  # You should change this\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "print('Predicted price of a 1650 sq-ft, 3 br house (using normal equations): ${}'.format(price))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e4d63ec2985b8afe7fc8fe7e7be03ad6689ea43c146cf0eb4cc362afcb3b8c1f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
